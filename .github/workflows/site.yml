name: Build site

on:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"       # hourly; adjust as needed
  push:
    branches: [ main ]

permissions:
  contents: read
  pages: write
  id-token: write

env:
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  SUMMARIZER_MODEL: gpt-4o-mini
  PDF_SUMMARY_MAX_PAGES: "30"
  PDF_SUMMARY_MAX_CHARS: "72000"
  PDF_SUMMARY_MAX_BULLETS: "16"
  PDF_SUMMARY_DEBUG: "1"

jobs:
  scrape:
    runs-on: ubuntu-24.04
    env:
      SALIDA_CIVICCLERK_URL: https://salidaco.civicclerk.com
      SALIDA_CIVICCLERK_ALT_HOSTS: "https://salidaco.civicclerk.com,https://cityofsalida.civicclerk.com"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          elif [ -f scraper/requirements.txt ]; then
            pip install -r scraper/requirements.txt
          else
            echo "requirements.txt not found" && exit 1
          fi

      - name: Install Playwright browsers
        run: python -m playwright install --with-deps chromium

      - name: Print effective PDF limits
        run: |
          python - <<'PY'
          import os
          print("ENV.PDF_SUMMARY_MAX_BULLETS =", os.getenv("PDF_SUMMARY_MAX_BULLETS"))
          print("ENV.PDF_SUMMARY_MAX_PAGES   =", os.getenv("PDF_SUMMARY_MAX_PAGES"))
          print("ENV.PDF_SUMMARY_MAX_CHARS   =", os.getenv("PDF_SUMMARY_MAX_CHARS"))
          PY

      - name: Clear agenda summary cache
        run: rm -rf data/cache/agenda_summaries || true

      - name: Debug hosts
        run: |
          echo "SALIDA_CIVICCLERK_URL=${SALIDA_CIVICCLERK_URL}"
          echo "SALIDA_CIVICCLERK_ALT_HOSTS=${SALIDA_CIVICCLERK_ALT_HOSTS}"

      - name: Run scraper
        run: |
          python -m scraper.main
          test -d data && ls -la data || (echo "no data dir generated" && exit 1)

      # --- Summarization step (this is what produces the bullets) ---
      - name: Summarize agendas (GPT)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          ( python -m scraper.summarize --input data/meetings.json --out data/cache/agenda_summaries ) || python -m scraper.summarize
          echo "---- agenda_summaries ----"
          ls -la data/cache/agenda_summaries || true

      - name: Sanity check: did we produce any bullets?
        run: |
          python -c "import json,glob,sys; b=0
for p in glob.glob('data/cache/agenda_summaries/*.json'):
    try:
        obj=json.load(open(p))
        b+=1 if obj.get('bullets') else 0
    except Exception:
        pass
print(f'Found {b} items with bullets'); sys.exit(0 if b>0 else 1)"

      - name: Upload data artifact
        uses: actions/upload-artifact@v4
        with:
          name: scrape-data
          path: data/
          if-no-files-found: warn
