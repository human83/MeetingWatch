name: Build site

on:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"     # hourly; adjust if you want
  push:
    branches: [ main ]

permissions:
  contents: read
  pages: write
  id-token: write

env:
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  SUMMARIZER_MODEL: gpt-4o-mini
  PDF_SUMMARY_MAX_PAGES: "30"
  PDF_SUMMARY_MAX_CHARS: "72000"
  PDF_SUMMARY_MAX_BULLETS: "16"
  PDF_SUMMARY_DEBUG: "1"

jobs:
  scrape:
    runs-on: ubuntu-24.04
    env:
      SALIDA_CIVICCLERK_URL: https://salidaco.civicclerk.com
      SALIDA_CIVICCLERK_ALT_HOSTS: "https://salidaco.civicclerk.com,https://cityofsalida.civicclerk.com"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          elif [ -f scraper/requirements.txt ]; then
            pip install -r scraper/requirements.txt
          else
            echo "requirements.txt not found" && exit 1
          fi

      - name: Install Playwright browsers
        run: python -m playwright install --with-deps chromium

      - name: Print effective PDF limits
        run: |
          python - <<'PY'
          import os
          print("ENV.PDF_SUMMARY_MAX_BULLETS =", os.getenv("PDF_SUMMARY_MAX_BULLETS"))
          print("ENV.PDF_SUMMARY_MAX_PAGES   =", os.getenv("PDF_SUMMARY_MAX_PAGES"))
          print("ENV.PDF_SUMMARY_MAX_CHARS   =", os.getenv("PDF_SUMMARY_MAX_CHARS"))
          PY

      - name: Clear agenda summary cache
        run: rm -rf data/cache/agenda_summaries || true

      - name: Debug hosts
        run: |
          echo "SALIDA_CIVICCLERK_URL=${SALIDA_CIVICCLERK_URL}"
          echo "SALIDA_CIVICCLERK_ALT_HOSTS=${SALIDA_CIVICCLERK_ALT_HOSTS}"

      - name: Run scraper
        run: |
          python -m scraper.main
          test -d data && ls -la data || (echo "no data dir generated" && exit 1)

       # === Summarization step (produces the bullets) ===
      - name: Summarize agendas (GPT, fresh & verbose)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_LOG: debug            # surface SDK errors
          PYTHONWARNINGS: default
          PDF_SUMMARY_DEBUG: "1"
        run: |
          set -euo pipefail
          # show how many meetings have any doc-like URLs the summarizer could use
          python - <<'PY'
          import json, sys
          path = "data/meetings.json"
          with open(path, "r") as f:
            items = json.load(f)
          def has_doc(x):
            keys = ("agenda_url","agenda_pdf","packet_url","packet_pdf","attachments","documents")
            return any(k in x and x[k] for k in keys)
          n_with_docs = sum(1 for x in items if has_doc(x))
          print(f"[diagnostic] meetings total={len(items)} with_docs={n_with_docs}")
          if n_with_docs == 0:
            print("[diagnostic] None of the meetings have document links; summarizer will produce empty outputs.", file=sys.stderr)
          PY

          # start clean to avoid picking up stale stubs
          rm -rf data/cache/agenda_summaries
          mkdir -p data/cache/agenda_summaries

          # Try explicit CLI first (fresh run); if your module supports flags like --no-cache, add them here.
          # If that command isn't implemented, fall back to default module run.
          ( python -m scraper.summarize --input data/meetings.json --out data/cache/agenda_summaries ) || true
          ( python -m scraper.summarize ) || true

          echo "---- agenda_summaries (ls) ----"
          ls -la data/cache/agenda_summaries || true

          echo "---- sample outputs (head) ----"
          for f in $(ls -1 data/cache/agenda_summaries/*.json 2>/dev/null | head -n 3); do
            echo ">>> $f"
            head -c 600 "$f" || true
            echo
            echo "-----"
          done

          echo "---- grep for errors ----"
          (grep -Hni '"error"\|"exception"\|"traceback"' data/cache/agenda_summaries/*.json || true)

      - name: "Sanity check: did we produce any bullets?"
        run: |
          set -e
          n=$(grep -l '"bullets"' data/cache/agenda_summaries/*.json 2>/dev/null | wc -l || true)
          echo "Files with bullets: ${n}"
          test "${n}" -gt 0

      - name: Upload data artifact
        uses: actions/upload-artifact@v4
        with:
          name: scrape-data
          path: data/
          if-no-files-found: warn
