name: Scrape + Build + Deploy

on:
  schedule:
    - cron: "0 * * * *"      # every hour; tweak as needed
  workflow_dispatch:         # manual run from the Actions tab
  push:
    branches: [ main ]       # optional: run on push too

permissions:
  contents: read
  pages: write
  id-token: write

env:
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  SUMMARIZER_MODEL: gpt-4o-mini
  PDF_SUMMARY_MAX_PAGES: '20'
  PDF_SUMMARY_MAX_CHARS: '24000'

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        run: |
          python -m scraper.main
          test -d data && ls -la data || (echo "no data dir generated" && exit 1)

      - name: Upload data artifact (data/)
        uses: actions/upload-artifact@v4
        with:
          name: scrape-data
          path: data/

  build:
    runs-on: ubuntu-latest
    needs: scrape
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download scrape data
        uses: actions/download-artifact@v4
        with:
          name: scrape-data
          path: ./data

      - name: Prepare static site
        run: |
          mkdir -p _site
          cp -r data _site/
          # copy any other static files/assets your site needs here
          # cp -r public/* _site/  # (example)

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: _site

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
